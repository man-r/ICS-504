{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66342c4",
   "metadata": {},
   "source": [
    "# ICS-504 Deep learning Assignment 02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b5416",
   "metadata": {},
   "source": [
    "Face recognition can be categorized into face classification and face verification. Given an image of a personâ€™s face, the task of classifying the ID of the face is known as face classification, which is a closed-set problem. The task of determining whether two face images are of the same person is known as face verification, which is an open-set problem1.\n",
    "\n",
    "In this assignment, you will use Convolutional Neural Networks (CNNs) to design an end-to-end system for face classification/identification. Your system will be given an image as input and will output the ID/name of the person shown in that image.\n",
    "\n",
    "\n",
    "You will train your model on a dataset with a few thousand of images (i.e., a set of images, each labeled by an ID that uniquely identifies the person). Use Jupyter notebook to show each of the following steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0303f",
   "metadata": {},
   "source": [
    "## Part 01 [80 points]\n",
    "\n",
    "1. Prepare the data by\n",
    "\n",
    "    1.1.Preprocessing the data by zero-centering it \n",
    "    \n",
    "    1.2.Dividing the train data into train (80%) and validation (20%)\n",
    "\n",
    "\n",
    "2. Implement the following model in Pytorch. The specifications of each layer are given under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d090d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46512132",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "load the data using ImageFolder and transforming and resize to 64x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6789db3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 4298\n",
      "    Root location: ./FaceDataset/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "           )\n",
      "Test data:\n",
      "Dataset ImageFolder\n",
      "    Number of datapoints: 100\n",
      "    Root location: ./FaceDataset/test\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),# Resize the images to 64x64\n",
    "    transforms.ToTensor() # Turn the image into a torch.Tensor\n",
    "])\n",
    "\n",
    "# loading training data\n",
    "train_data = datasets.ImageFolder(root='./FaceDataset/train', # target folder of images\n",
    "                                  transform=data_transform, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "# loading test data\n",
    "test_data = datasets.ImageFolder(root='./FaceDataset/test', \n",
    "                                 transform=data_transform)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d078b4",
   "metadata": {},
   "source": [
    "we can see that we have 4298 training data and 100 test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ac95a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n000003': 0,\n",
       " 'n000010': 1,\n",
       " 'n000011': 2,\n",
       " 'n000013': 3,\n",
       " 'n000015': 4,\n",
       " 'n000017': 5,\n",
       " 'n000018': 6,\n",
       " 'n000020': 7,\n",
       " 'n000023': 8,\n",
       " 'n000024': 9,\n",
       " 'n000025': 10,\n",
       " 'n000028': 11,\n",
       " 'n000032': 12,\n",
       " 'n000033': 13,\n",
       " 'n000034': 14,\n",
       " 'n000035': 15,\n",
       " 'n000038': 16,\n",
       " 'n000041': 17,\n",
       " 'n000042': 18,\n",
       " 'n000046': 19,\n",
       " 'n000048': 20,\n",
       " 'n000049': 21,\n",
       " 'n000051': 22,\n",
       " 'n000053': 23,\n",
       " 'n000054': 24,\n",
       " 'n000057': 25,\n",
       " 'n000059': 26,\n",
       " 'n000062': 27,\n",
       " 'n000064': 28,\n",
       " 'n000065': 29,\n",
       " 'n000070': 30,\n",
       " 'n000072': 31,\n",
       " 'n000074': 32,\n",
       " 'n000075': 33,\n",
       " 'n000077': 34,\n",
       " 'n000086': 35,\n",
       " 'n000088': 36,\n",
       " 'n000089': 37,\n",
       " 'n000093': 38,\n",
       " 'n000101': 39,\n",
       " 'n000108': 40,\n",
       " 'n000110': 41,\n",
       " 'n000111': 42,\n",
       " 'n000113': 43,\n",
       " 'n000115': 44,\n",
       " 'n000118': 45,\n",
       " 'n000119': 46,\n",
       " 'n000120': 47,\n",
       " 'n000121': 48,\n",
       " 'n000125': 49}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get class names as a list\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbe4d25",
   "metadata": {},
   "source": [
    "we can see that we have 50 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e9dcc6",
   "metadata": {},
   "source": [
    "# Zero centering\n",
    "1. 1st we need to calculate the mean and the standar deviation of the training dataset\n",
    "2. reload the data using the mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "008e418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train mean tensor([0.4811, 0.4057, 0.3730])\n",
      "train std tensor([0.2601, 0.2350, 0.2283])\n"
     ]
    }
   ],
   "source": [
    "### train data\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in train_data:\n",
    "    mean += images.mean(dim=(1,2))\n",
    "    std += images.std(dim=(1,2))\n",
    "\n",
    "# Compute the mean and standard deviation of the pixel values for each channel\n",
    "mean /= len(train_data)\n",
    "std /= len(train_data)\n",
    "\n",
    "print(\"train mean\", mean)\n",
    "print('train std', std)\n",
    "\n",
    "# train mean tensor([0.4811, 0.4057, 0.3730])\n",
    "# train std tensor([0.2601, 0.2350, 0.2283])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4793b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),# Resize the images to 64x64\n",
    "    transforms.ToTensor(), # Turn the image into a torch.Tensor\n",
    "    transforms.Normalize(\n",
    "        mean=mean,\n",
    "        std=std\n",
    "    )\n",
    "])\n",
    "\n",
    "# loading training data\n",
    "train_data = datasets.ImageFolder(root='./FaceDataset/train', # target folder of images\n",
    "                                  transform=data_transform, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "# loading test data\n",
    "test_data = datasets.ImageFolder(root='./FaceDataset/test', transform=data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0426ffd",
   "metadata": {},
   "source": [
    "# Dividing the train data into train (80%) and validation (20%)\n",
    "split the data and then load them into a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33501a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoader's with batch size 32 and 8 workers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x21033eb1370>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x21033ecc310>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x21033ecc610>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "train_subdata, val_subdata = random_split(train_data, [int(len(train_data)*0.8), len(train_data)-int(len(train_data)*0.8)])\n",
    "\n",
    "# Setup batch size and number of workers \n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n",
    "\n",
    "# Create DataLoader's\n",
    "train_dataloader = DataLoader(train_subdata, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     num_workers=NUM_WORKERS)\n",
    "\n",
    "val_dataloader = DataLoader(val_subdata, \n",
    "                                     batch_size=BATCH_SIZE, \n",
    "                                     shuffle=True, \n",
    "                                     num_workers=NUM_WORKERS)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, \n",
    "                                    batch_size=BATCH_SIZE, \n",
    "                                    shuffle=False, \n",
    "                                    num_workers=NUM_WORKERS)\n",
    "\n",
    "train_dataloader, test_dataloader, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1025a969",
   "metadata": {},
   "source": [
    "# Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1239d395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNmodel(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(32, 16, kernel_size=(7, 7), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv_block_3): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_4): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv_block_5): Sequential(\n",
       "    (0): Dropout2d(p=0.5, inplace=False)\n",
       "  )\n",
       "  (conv_block_6): Sequential(\n",
       "    (0): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (conv_block_7): Sequential(\n",
       "    (0): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6400, out_features=1024, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=1024, out_features=50, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNNmodel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture copying TinyVGG from: \n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        #1st block CNN with 32 filters size 7x7 strid 1 no padding with relu activation function\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,out_channels=32,kernel_size=7, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #2nd block CNN with 16 filters size 7x7 strid 1 no padding with relu activation function\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32,out_channels=16,kernel_size=7, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #3dr block pooling 2x2 and 2 stride\n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        #4th block CNN 16 filters 5x5 stride 1 padding 1 with relu activation function\n",
    "        self.conv_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,out_channels=16,kernel_size=5, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        #5th block dropout p=0.5\n",
    "        self.conv_block_5 = nn.Sequential(\n",
    "            nn.Dropout2d(p=0.5)\n",
    "        )\n",
    "        \n",
    "        #6th block CNN 16 filters 5x5 strid 1 padding 0 with relu activation function\n",
    "        self.conv_block_6 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,out_channels=16,kernel_size=5, stride=1, padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        #6th block batch normalization\n",
    "        self.conv_block_7 = nn.Sequential(\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "        \n",
    "        #7th block fully connected NN \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from? \n",
    "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=6400,out_features=1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=1024,out_features=output_shape)\n",
    "        )\n",
    "        \n",
    "        #8th block is included in the cross entropy \n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_3(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_4(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_5(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_6(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "        \n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNmodel(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15542f5f",
   "metadata": {},
   "source": [
    "## testing the model using singal image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb5f958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single image shape: torch.Size([1, 3, 64, 64])\n",
      "\n",
      "Output logits:\n",
      "tensor([[-0.0491, -0.0155, -0.0406,  0.0256,  0.0178,  0.0076, -0.0121, -0.0138,\n",
      "         -0.0032, -0.0183, -0.0100, -0.0030,  0.0226,  0.0338,  0.0060,  0.0187,\n",
      "         -0.0204,  0.0063, -0.0081, -0.0262,  0.0133,  0.0255, -0.0254, -0.0411,\n",
      "         -0.0019, -0.0282,  0.0033,  0.0263,  0.0025,  0.0329,  0.0096, -0.0201,\n",
      "          0.0206, -0.0208, -0.0350,  0.0014, -0.0232, -0.0104,  0.0148, -0.0030,\n",
      "         -0.0093,  0.0148, -0.0082,  0.0350,  0.0224,  0.0181, -0.0396, -0.0071,\n",
      "          0.0233,  0.0006]])\n",
      "\n",
      "Output prediction probabilities:\n",
      "tensor([[0.0191, 0.0197, 0.0192, 0.0206, 0.0204, 0.0202, 0.0198, 0.0198, 0.0200,\n",
      "         0.0197, 0.0198, 0.0200, 0.0205, 0.0207, 0.0202, 0.0204, 0.0196, 0.0202,\n",
      "         0.0199, 0.0195, 0.0203, 0.0205, 0.0195, 0.0192, 0.0200, 0.0195, 0.0201,\n",
      "         0.0206, 0.0201, 0.0207, 0.0202, 0.0196, 0.0204, 0.0196, 0.0193, 0.0201,\n",
      "         0.0196, 0.0198, 0.0203, 0.0200, 0.0198, 0.0203, 0.0199, 0.0207, 0.0205,\n",
      "         0.0204, 0.0193, 0.0199, 0.0205, 0.0200]])\n",
      "\n",
      "Output prediction label:\n",
      "tensor([43])\n",
      "\n",
      "Actual label:\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "# 1. Get a batch of images and labels from the DataLoader\n",
    "img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
    "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "# 3. Perform a forward pass on a single image\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    pred = model(img_single.to(device))\n",
    "    \n",
    "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
    "print(f\"Output logits:\\n{pred}\\n\")\n",
    "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea07206",
   "metadata": {},
   "source": [
    "# training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65a7464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module, \n",
    "               dataloader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7043697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval() \n",
    "    \n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "    \n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "    \n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "            \n",
    "    # Adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b43335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module, \n",
    "          train_dataloader: torch.utils.data.DataLoader, \n",
    "          test_dataloader: torch.utils.data.DataLoader, \n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "          epochs: int = 5):\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "    \n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "        \n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd12f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.022004365921020508,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 60,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a3f8b4e2334abd839735cd3ec0032f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 3.8573 | train_acc: 0.0379 | test_loss: 3.7794 | test_acc: 0.0312\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 60\n",
    "\n",
    "# Recreate an instance of TinyVGG\n",
    "model = CNNmodel(input_shape=3, # number of color channels (3 for RGB) \n",
    "                  output_shape=len(train_data.classes)).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss() #include softmax\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0 \n",
    "model_results = train(model=model, \n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn, \n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")\n",
    "\n",
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e5b407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
